{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15842883",
   "metadata": {},
   "source": [
    "# Model Configuration - 42 Seoul Pass/Fail Prediction\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9251d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1589023",
   "metadata": {},
   "source": [
    "## 0. Collecting data and Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5585f",
   "metadata": {},
   "source": [
    "### 0-1. Calling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7911d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드에서 가져온 부분\n",
    "UID = \"u-s4t2ud-74766556a4e743fc75b26498d8f22e2c6444da37b87ddf898ea7fe883a3ddfd3\"\n",
    "SECRET = \"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\"\n",
    "BASE_URL = \"https://api.intra.42.fr\"\n",
    "\n",
    "auth = (UID, SECRET)\n",
    "response = requests.post(f\"{BASE_URL}/oauth/token\", auth=auth, data={\n",
    "    'grant_type': 'client_credentials',\n",
    "    'client_id': UID,\n",
    "    'client_secret': SECRET\n",
    "})\n",
    "response.raise_for_status()\n",
    "token_data = response.json()\n",
    "access_token = token_data[\"access_token\"]\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"page[size]\": \"100\",\n",
    "    \"page[number]\": \"1\"\n",
    "    \n",
    "}\n",
    "\n",
    "def get_as_corrector(user_id, page_number):\n",
    "    params[\"page[number]\"] = str(page_number)\n",
    "    endpoint = f\"/v2/users/{user_id}/scale_teams/as_corrector\"\n",
    "    response = requests.get(f\"{BASE_URL}{endpoint}\", headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_as_corrected(user_id, page_number):\n",
    "    params[\"page[number]\"] = str(page_number)\n",
    "    endpoint = f\"/v2/users/{user_id}/scale_teams/as_corrected\"\n",
    "    response = requests.get(f\"{BASE_URL}{endpoint}\", headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def count_word_occurrences(json_data, word):\n",
    "    return json_data.lower().count(word)\n",
    "\n",
    "def process_user(user_id):\n",
    "    # \"/v2/users/{user_id}/scale_teams/as_corrector\" 엔드포인트에 대한 요청\n",
    "    response_corrector1 = get_as_corrector(user_id, 1)\n",
    "    # \"/v2/users/{user_id}/scale_teams/as_corrected\" 엔드포인트에 대한 요청\n",
    "    response_corrected1 = get_as_corrected(user_id, 1)\n",
    "    # JSON 응답을 가져와서 단어 개수 세기\n",
    "    corrector_count = count_word_occurrences(json.dumps(response_corrector1), \"corrector\")\n",
    "    corrected_count = count_word_occurrences(json.dumps(response_corrected1), \"correcteds\")\n",
    "    \n",
    "    response_corrector2 = get_as_corrector(user_id, 2)\n",
    "    response_corrected2 = get_as_corrected(user_id, 2)\n",
    "    corrector_count += count_word_occurrences(json.dumps(response_corrector2), \"corrector\")\n",
    "    corrected_count += count_word_occurrences(json.dumps(response_corrected2), \"corrected\")\n",
    "\n",
    "    return {\n",
    "        \"id\": user_id,\n",
    "        \"corrector\": corrector_count,\n",
    "        \"corrected\": corrected_count\n",
    "   }\n",
    "\n",
    "# CSV 파일 작성 함수\n",
    "def append_to_csv(data, filename):\n",
    "    with open(filename, \"a\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"id\", \"corrector\", \"corrected\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        # 각 사용자에 대한 처리\n",
    "        # CSV 파일에 데이터 작성\n",
    "        writer.writerow(data)\n",
    "        print(f\"{data['id']} is appended\")\n",
    "\n",
    "# 유저 리스트 받아오기\n",
    "def read_user_ids_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # 첫 번째 열의 데이터를 읽어옴 (헤더 제외)\n",
    "    user_ids = df.iloc[:, 0].tolist()\n",
    "    return user_ids\n",
    "\n",
    "user_ids = read_user_ids_from_csv(\"/Users/kimkangmin/user_data_campus_29_for_feedback.csv\")\n",
    "\n",
    "for user_id in user_ids:\n",
    "    result_correct = process_user(user_id)\n",
    "    append_to_csv(result_correct, \"/Users/kimkangmin/user_feedback_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ee29d",
   "metadata": {},
   "source": [
    "### 0-2. Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d2412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "812a704d",
   "metadata": {},
   "source": [
    "코드추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a4ae3",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "### 1-1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332b8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/kimkangmin/kangmin/공부/3학년 2학기/Business Analytics/Team Project/modelforteamproject.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002329a",
   "metadata": {},
   "source": [
    "## drop the data points whose final exam score is under 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e19cd245",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Final exam score'] >= 42]\n",
    "X = data.drop('PASS', axis = 1)\n",
    "y = data['PASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f803e6e",
   "metadata": {},
   "source": [
    "### 1-2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4557be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aab5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f35b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a26aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e03be3",
   "metadata": {},
   "source": [
    "### 1-3. Treating Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fb108b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([210, 321]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5b667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3f736b",
   "metadata": {},
   "source": [
    "## 2. Data Mining with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Define the pipeline with imbalanced-learn\n",
    "pipe = ImbPipeline([\n",
    "    ('preprocessing', None),\n",
    "    ('sampler', None),  # Added for oversampling/undersampling\n",
    "    ('classifier', None)\n",
    "])\n",
    "hyperparam_grid = [\n",
    "    {\n",
    "        'preprocessing': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None],\n",
    "        'classifier': [LogisticRegression(max_iter=1000)],\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None]\n",
    "    },\n",
    "    {\n",
    "        'preprocessing': [None],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None],\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__max_features': [1, 2, 3, 4, 5],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None]\n",
    "    },\n",
    "    {\n",
    "        'preprocessing': [StandardScaler(), MinMaxScaler(), None],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None],\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [1, 3, 5, 10, 15],\n",
    "        'classifier__metric': ['minkowski', 'euclidean', 'mahalanobis'],\n",
    "        'sampler': [RandomOverSampler(), RandomUnderSampler(), None]\n",
    "    }\n",
    "]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(pipe, hyperparam_grid, scoring='roc_auc', refit=True, cv=kfold)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "print(f\"Test-set score: {grid_search.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca7d6c",
   "metadata": {},
   "source": [
    "Todo: ROC curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca6f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbcd103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
